# Logstash Configuration for Cardano OTC Trading System
# Centralized log processing and enrichment

input {
  # Filebeat input for application logs
  beats {
    port => 5044
  }

  # Docker container logs
  gelf {
    port => 12201
    type => "docker"
  }

  # Syslog input for system logs
  syslog {
    port => 514
    type => "syslog"
  }

  # TCP input for structured logs
  tcp {
    port => 5000
    codec => json_lines
    type => "structured"
  }

  # HTTP input for webhooks/alerts
  http {
    port => 8080
    type => "webhook"
  }
}

filter {
  # Parse application logs
  if [type] == "otc-app" {
    # Parse JSON structured logs
    if [message] =~ /^\{.*\}$/ {
      json {
        source => "message"
      }
      
      # Add timestamp parsing
      if [timestamp] {
        date {
          match => [ "timestamp", "ISO8601" ]
          target => "@timestamp"
        }
      }
      
      # Parse log level
      if [level] {
        mutate {
          uppercase => [ "level" ]
          add_field => { "log_level" => "%{level}" }
        }
      }
      
      # Extract request ID for correlation
      if [requestId] {
        mutate {
          add_field => { "correlation_id" => "%{requestId}" }
        }
      }
      
      # Parse user information
      if [userId] {
        mutate {
          add_field => { "user_id" => "%{userId}" }
        }
      }
      
      # Parse IP address
      if [ip] {
        mutate {
          add_field => { "client_ip" => "%{ip}" }
        }
        
        # GeoIP lookup
        geoip {
          source => "client_ip"
          target => "geoip"
        }
      }
    } else {
      # Parse unstructured logs with grok
      grok {
        match => { 
          "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{WORD:level}\] %{GREEDYDATA:log_message}"
        }
      }
    }
  }

  # Parse database logs
  if [type] == "postgres" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{NUMBER:pid}\]: \[%{NUMBER:line_num}-%{NUMBER:session_line}\] user=%{WORD:db_user},db=%{WORD:database},app=%{DATA:application},client=%{IP:client_ip} %{WORD:log_level}: %{GREEDYDATA:log_message}"
      }
    }
    
    # Parse slow query logs
    if [log_message] =~ /duration:/ {
      grok {
        match => {
          "log_message" => "duration: %{NUMBER:query_duration:float} ms.*statement: %{GREEDYDATA:sql_query}"
        }
      }
      
      mutate {
        add_tag => [ "slow_query" ]
      }
    }
    
    # Parse connection logs
    if [log_message] =~ /connection/ {
      mutate {
        add_tag => [ "connection" ]
      }
    }
  }

  # Parse nginx access logs
  if [type] == "nginx-access" {
    grok {
      match => {
        "message" => "%{COMBINEDAPACHELOG} %{NUMBER:request_time:float} %{NUMBER:upstream_time:float}"
      }
    }
    
    # Parse response codes
    if [response] {
      if [response] >= 200 and [response] < 300 {
        mutate { add_tag => [ "success" ] }
      } else if [response] >= 300 and [response] < 400 {
        mutate { add_tag => [ "redirect" ] }
      } else if [response] >= 400 and [response] < 500 {
        mutate { add_tag => [ "client_error" ] }
      } else if [response] >= 500 {
        mutate { add_tag => [ "server_error" ] }
      }
    }
    
    # GeoIP lookup for client IP
    if [clientip] {
      geoip {
        source => "clientip"
        target => "geoip"
      }
    }
  }

  # Parse nginx error logs
  if [type] == "nginx-error" {
    grok {
      match => {
        "message" => "%{DATESTAMP:timestamp} \[%{WORD:log_level}\] %{NUMBER:pid}#%{NUMBER:tid}: %{GREEDYDATA:log_message}"
      }
    }
  }

  # Parse Redis logs
  if [type] == "redis" {
    grok {
      match => {
        "message" => "%{NUMBER:pid}:%{WORD:role} %{DATESTAMP:timestamp} %{WORD:log_level} %{GREEDYDATA:log_message}"
      }
    }
  }

  # Parse Docker container logs
  if [type] == "docker" {
    # Extract container information
    if [container_name] {
      if [container_name] =~ /otc-app/ {
        mutate { add_field => { "service" => "application" } }
      } else if [container_name] =~ /otc-postgres/ {
        mutate { add_field => { "service" => "database" } }
      } else if [container_name] =~ /otc-redis/ {
        mutate { add_field => { "service" => "cache" } }
      } else if [container_name] =~ /otc-nginx/ {
        mutate { add_field => { "service" => "proxy" } }
      }
    }
  }

  # Security-focused parsing
  if [log_message] {
    # Detect potential SQL injection attempts
    if [log_message] =~ /(?i)(union|select|insert|update|delete|drop|exec|script)/ {
      mutate {
        add_tag => [ "potential_sqli" ]
        add_field => { "security_event" => "sql_injection_attempt" }
      }
    }
    
    # Detect XSS attempts
    if [log_message] =~ /(?i)(<script|javascript:|onload=|onerror=)/ {
      mutate {
        add_tag => [ "potential_xss" ]
        add_field => { "security_event" => "xss_attempt" }
      }
    }
    
    # Detect brute force attempts
    if [log_message] =~ /(?i)(failed|invalid|unauthorized|forbidden).*(?i)(login|auth|password)/ {
      mutate {
        add_tag => [ "auth_failure" ]
        add_field => { "security_event" => "authentication_failure" }
      }
    }
  }

  # Business logic parsing
  if [type] == "otc-app" {
    # Parse transaction events
    if [log_message] =~ /transaction/ {
      mutate { add_tag => [ "transaction" ] }
      
      # Extract transaction ID
      grok {
        match => {
          "log_message" => ".*transaction.*id[:\s]+(?<transaction_id>[a-fA-F0-9]{64})"
        }
        tag_on_failure => []
      }
    }
    
    # Parse wallet events
    if [log_message] =~ /wallet/ {
      mutate { add_tag => [ "wallet" ] }
    }
    
    # Parse signing events
    if [log_message] =~ /sign/ {
      mutate { add_tag => [ "signing" ] }
    }
    
    # Parse UTxO events
    if [log_message] =~ /utxo/ {
      mutate { add_tag => [ "utxo" ] }
    }
  }

  # Add common fields
  mutate {
    add_field => {
      "environment" => "production"
      "system" => "otc-trading"
      "processed_at" => "%{@timestamp}"
    }
  }

  # Clean up unwanted fields
  mutate {
    remove_field => [ "beat", "offset", "prospector" ]
  }

  # Add fingerprint for deduplication
  fingerprint {
    source => ["message", "host", "type"]
    target => "[@metadata][fingerprint]"
    method => "MURMUR3"
  }
}

output {
  # Main Elasticsearch output
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "otc-logs-%{+YYYY.MM.dd}"
    template_name => "otc-logs"
    template_pattern => "otc-logs-*"
    template => "/usr/share/logstash/templates/otc-logs-template.json"
    document_id => "%{[@metadata][fingerprint]}"
    
    # Authentication if enabled
    # user => "elastic"
    # password => "changeme"
  }

  # Security events to separate index
  if "security_event" in [security_event] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "otc-security-%{+YYYY.MM.dd}"
      template_name => "otc-security"
      template_pattern => "otc-security-*"
    }
  }

  # High-priority alerts to different output
  if [log_level] == "ERROR" or [log_level] == "FATAL" {
    # Send to alerting system
    http {
      url => "http://alertmanager:9093/api/v1/alerts"
      http_method => "post"
      format => "json"
      content_type => "application/json"
    }
    
    # Also send to Slack/Discord webhook if configured
    if "${ALERT_WEBHOOK_URL}" {
      http {
        url => "${ALERT_WEBHOOK_URL}"
        http_method => "post"
        format => "json"
        content_type => "application/json"
        mapping => {
          "text" => "ðŸš¨ OTC System Alert: %{log_level} - %{log_message}"
          "username" => "OTC Monitor"
          "channel" => "#alerts"
        }
      }
    }
  }

  # Performance metrics to dedicated index
  if "slow_query" in [tags] or [request_time] > 2.0 {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "otc-performance-%{+YYYY.MM.dd}"
    }
  }

  # Debug output (disable in production)
  # stdout { 
  #   codec => rubydebug 
  # }
}